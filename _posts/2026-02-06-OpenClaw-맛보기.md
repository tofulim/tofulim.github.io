---
categories:
  - ml
tags:
  - MCP
  - Clawdbot
  - OpenClaw
  - moltbot
mermaid: true
image: assets/img/260206_openclaw.png
---
> 유행의 돌풍! Clawdbot! 아니 이제는 moltbot! 아니 바로 OpenClaw에 대해 알아보자!
---

# 이름과 인기
초기 이름은 Clawdbot이었으나.. 당연히 Anthropic에서 claude와 유사하니 문제삼았다.

때문에 이름을 moltbot으로 변경하였다. 그러나 moltbot에 대한 상표권을 등록할 수 없어서 이름을 다시 바꾸었다.

그렇게 최종적으로 Opensource임을 강조하면서 랍스터 캐릭터를 유지할 수 있는 **OpenClaw**가 되었다!

![](https://i.imgur.com/fA4GxW4.png)

애플 시리가 지향했던 행동하는 AI 비서를 지향한다.

![](https://i.imgur.com/oqOAzPE.png)

github star가 16만개에 이르고 google trend에서도 급격히 오르고 있다.
OpenClaw로 며칠전부터 뜨는 것인데.. 같이 검색된 검색어를 보면 이름을 여러번 바꿨기에 그런 것을 알 수 있다. ㅋㅋ

![](https://i.imgur.com/z2I2HIa.png)


메인 슬로건은 행동하는 AI로 일상 업무를 기존 채팅앱에 통합했다고 한다.

장점으로 내세우는 것들은 다음과 같다.
![](https://i.imgur.com/mi4qYxv.png)

- 장비 상관 x
- 기존 채팅 앱과의 연동
- 오래 context를 기억하며
- 인터넷을 돌아다니고
- 장비를 온전히 다루며
- 다양한 확장성을 가진다
-> 기존 AI 서비스나 툴의 좋은점을 모두 합친 느낌이다.

## Any Chat App?
![](https://i.imgur.com/EUd9SpG.png)

### 커뮤니티에 공유된 활용 사례
![](https://i.imgur.com/7lJQgRF.png)

- 믿기 어려운 여러 사례가 있지만 박스친 사례들은 실제로 linked이나 커뮤니티에서 내가 보기도 했고 Slack 사례의 경우 메신저의 특성을 살릴 수도 있어서 신기했다.

내가 linkedin에서 본 사례는 다음이었다.
![](https://i.imgur.com/MQokR7u.png)

![](https://i.imgur.com/gBMtE2p.png)

- openclaw를 구축하며 사용해보니 이분의 의견에 많이 공감됐다.

# OpenClaw의 구조
### 어떤 구조이길래 가능할까?
![](https://i.imgur.com/vhjg4uF.png)

## 편리한 장점
- 챗봇 대화 히스토리/세션 문제 해결
	- 이게 참 귀찮은 문제인데 편하게 제공해준다.
- 편리한 신규 Agent 추가

### 챗봇 대화 히스토리/세션 문제 해결
![](https://i.imgur.com/E6duAqg.jpeg)

- 세션별 대화를 json으로 보관하며 세션별, 채녈별로 다른 모델을 선택해 대화할 수 있다.
- 로컬모델 + 클라우드 모델 혼용도 가능하다.
	- 복잡하고 어려운 채팅 -> 클라우드 모델
	- 간단한 db 문답성 채팅 -> 로컬 모델

- 제공하는 웹 UI에서 세션과 챗을 보면 openclaw에서 관리해주고 메신져 채널들은 단순 질의응답을 미러링하는 것임을 알 수 있다.
	- ![](https://i.imgur.com/k6MacsG.png)
	- ![](https://i.imgur.com/0pMzjB3.png)

### 편리한 신규 Agent 추가
![](https://i.imgur.com/MXNRCF6.png)

![](https://i.imgur.com/7CDICjP.png)

- Agent와 Workspace는 1:1로 대응하며 markdown으로 Agent를 표현하면 된다. (SystemPrompt)
	- 나의 경우에는 내 목표를 설명하면서 chatgpt에게 만들어달라고 했다.
	- ![](https://i.imgur.com/UQ9poak.png)

## Markdown으로 스스로를 인지하고 외부 지식을 사용하는 방법
![](https://i.imgur.com/VkhDIX2.png)

- Agent 정의 markdown파일로 본인 스스로를 인지한다.
- 외부 지식은 별도 디렉토리에 markdown으로 제공하고 agent 정의 마크다운에서 이를 활용해 답변하라고 지시한다.
	- 추가 외부 지식은 embedding 모델에 의해 chunking 되어 agent만의 local sqlite DB에 저장된다.
	- memory_search 툴을 사용할 때 RAG처럼 검색 사용한다.

# 데모 - 나의 커스텀 비서 (Obsidian)
- 나는 obsidian으로 내 일상을 기록한다. 태그를 통해 #헬스, #러닝, #술, #샐러드 등 기록을 남겨 놓는다. 
- 내 로컬 Obsidian Vault에 저장된 일단위, 주단위 월단위 기록들을 보고 OpenClaw가 내 개인 건강 비서가 되면 좋겠다고 생각했다.
	- ex1) 지난 달에 비해 운동량이 30% 부족해요! 특히 러닝이 줄었어요 건강에 유의하세요~
	- ex2) 지난 주에 비해 샐러드 섭취량이 3회 늘었어요! 
## Agent 설정
- markdown에 성격을 비롯한 여러 사항을 구현해 놓는다.
	- Agent 설정 markdown 파일
		- 너는 zed의 Life Assistant
		- zed는 건강에 관심이 많음
		- Obsidian에 기록된 사용자 일상 활용 (#태그)
		- ...
	- (obsidian 관련 cli, tool, skill 연동하지 않고 그냥 local obisidan 파일들의 경로만 제공)
![](https://i.imgur.com/3NMr92R.png)

- 물론 이것도 copilot으로 만들었다.

### 데모 화면 (good)
- engine
	- Claude Sonet 4.0
- embedding model
	- Qwen3.0-embedding
		(모델선택에 큰 의미는 없다. Ollama local LLM으로 엔진 테스트할 때 Qwen3 써서 그렇다.)

#### Slack연동한 채팅
![](https://i.imgur.com/0xiK6bP.png)

- 내가 원하는대로 잘 응답해주었다.

제공한 사용자 메뉴얼은 다음과 같았다.
- ![](https://i.imgur.com/2ZekSNd.png)

# OpenClaw 후기
## 인상 정리
- MCP보다 한 단계 더 편해졌다
	- MCP가 LLM에 손발을 달아주는 방식이라면 
	- OpenClaw는 이미 손발이 달린 LLM을 내 책상에 앉혀두고 나는 밖에 나가 메신저로 대화하는 느낌에 가깝다.
	- 완전히 새로운 기술은 아니다. 기존에도 충분히 구현 가능했던 구조들이다.
	- 혁신적인 신기술이 등장했다기보다는, 여러 도구와 패턴을 매우 잘 정리해 하나의 사용하기 쉬운 형태로 묶어준 느낌이다.
- 세팅과 통합의 부담을 크게 줄여준다
	- 예전에도 다음것들은 할 수 있었다.
		- LLM 서빙(chatgpt, claude, …) 로컬 LLM 서빙(Ollama)
		- 채팅 환경 구성
		- 도구 연동
	- OpenClaw는 이 과정에서 발생하던 설정·통합의 수고를 상당 부분 덜어준다.

그러나 토큰을 매우 많이 소모한다.
데모 용으로 4~5번 정도 채팅했는데 3천원(1.8$) 정도 소모했다. 하루종일 돌리면 어떨지..
![](https://i.imgur.com/F0UVKyQ.png)

토큰양은 다음과 같았다.
![](https://i.imgur.com/5hKK7uE.png)

## 일상 메신져에 도구 사용하는 LLM을 통합한다는 관점
![](https://i.imgur.com/PRcs41M.png)

- 로컬 LLM으로 서빙하는 경우
	- 성능: 허술하지만 어느정도 유저가 원하는 답변을 쿼리해 내놓을 수 있을 것 같다.
	- 한계
		- 맥북 M2 Pro 32G 기준으로는 응답이 매우 느림 (수분 정도 소요)
		- 동시 처리량 문제
- Cloud API 사용 서빙하는 경우
	- 성능: 유의미한 응대가 가능해보인다.
	- 한계
		- 성능만큼 가격이 비싸 낮은 모델로 어디까지 되는지 튜닝 및 실험 필요

## 한 턴에 비싼 이유
![](https://i.imgur.com/xWBe6PE.png)

- 세션의 대화 기록 내역을 보면 json형태로 유저와 봇이 주고받은 turn 대화가 있는데 여기에 봇이 응답을 내기까지 어떤 tool을 요청하였고 응답받고 재추론했는지 알 수 있다.

위 이미지는 이런 내용을 담고 있다.
- 유저 발화 1건과 봇 발화 1건
	- ![](https://i.imgur.com/0xiK6bP.png)
- 봇이 최종 발화를 도출하기위해 사용한 14개의 추론
	- 9건의 read tool 사용 (obsidian 문서 읽기)
	- 5건의 exec tool 사용 (기타 시스템 커맨드 실행 명령)

---
+본 포스팅은 내가 회사에서 테크 세미나 시간에 발표한 내용의 일부이다.
